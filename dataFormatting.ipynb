{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting Data for Submission\n",
    "\n",
    "In order to cut down on API requests & cut costs, one step that we considered was to read through the top 10 most popular jobs from our data, and omit that from our api submission. Instead, we categorize these jobs ourselves. To accomplish this, first locate which jobs are the most popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "def process_jsonl_folder(folder_path, output_file):\n",
    "    title_counter = Counter()\n",
    "    total_titles = 0\n",
    "    total_lines_read = 0\n",
    "\n",
    "    # Iterate through all files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.jsonl'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                for line in file:\n",
    "                    total_lines_read += 1\n",
    "                    try:\n",
    "                        record = json.loads(line.strip())\n",
    "                        title_name = record.get('title.name')\n",
    "                        if title_name:\n",
    "                            # Normalize to lowercase for case-insensitivity\n",
    "                            title_counter[title_name.lower()] += 1\n",
    "                            total_titles += 1\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Error decoding line in file {file_name}: {line.strip()}\")\n",
    "\n",
    "    # Get the top 100 titles by frequency, sorted in descending order\n",
    "    top_titles = title_counter.most_common(100)\n",
    "\n",
    "    # Write results to the output JSONL file\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "        for title, frequency in top_titles:\n",
    "            percentage = f\"{(frequency / total_titles) * 100:.2f}%\"\n",
    "            output_record = {\n",
    "                \"title.name\": title,\n",
    "                \"frequency\": frequency,\n",
    "                \"percentage\": percentage\n",
    "            }\n",
    "            out_file.write(json.dumps(output_record) + '\\n')\n",
    "\n",
    "    print(f\"Top 100 job titles written to {output_file}\")\n",
    "    print(f\"Total lines read from all input files: {total_lines_read}\")\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"path/to/folder/with/your/data\"  # Replace with your folder path\n",
    "output_file = \"path/to/jsonl/output/file\"   # Replace with your desired output file path\n",
    "process_jsonl_folder(folder_path, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code will find the 100 most popualar jobs across all provided data. We used the 10 most popular to filter from, but this can be changed as needed. Furthermore, when applying our filter, we associate each job with a position, either Entry, Mid, Senior, or Executive with a map. This is so that we have a seniority level to categorize the position with manually, and so that our manual data resembles what OpenAI will send us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Concatenation\n",
    "\n",
    "This is entirely optional depending on how your data was handled, but when worked on the project, our data was split into multiple partitions. However, for the next step, only a single file is used as input. Below is a simple concatentation script which will write all lines of jsonl files in a directory into a single new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def combine_jsonl_files():\n",
    "    # Hardcoded directory containing the jsonl files\n",
    "    directory = \"path/to/data/directory\"\n",
    "\n",
    "    # Name of the new file that will contain combined data\n",
    "    output_filename = \"combined.jsonl\"\n",
    "    output_filepath = os.path.join(directory, output_filename)\n",
    "\n",
    "    # Open the output file for writing\n",
    "    with open(output_filepath, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        # Loop through everything in the directory\n",
    "        for filename in os.listdir(directory):\n",
    "            # Only process .jsonl files and skip if it's the output file itself\n",
    "            if filename.endswith(\".jsonl\") and filename != output_filename:\n",
    "                print(f\"Writing data from {filename}\")\n",
    "                file_path = os.path.join(directory, filename)\n",
    "\n",
    "                # Read each line from the current file and write it to the output file\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "                    for line in infile:\n",
    "                        outfile.write(line)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    combine_jsonl_files()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for Submission\n",
    "\n",
    "In order for OpenAI to be able to interpret the data that we send them, we must create a series of jsonl files that adhere to how they attempt to read our data. The code below will do the following:\n",
    "\n",
    "- Separate data into 2 categories, those whose title.name is or isn't found in our popular job map\n",
    "- Partition unpopular job group into files containing at most 50000 lines (the maximum for OpenAI batch submissions)\n",
    "- Create custom ids for each unpopular job entry, based on their employee_id\n",
    "- Create a single jsonl file of the entries with popular jobs, who we manually analyzed\n",
    "\n",
    "Note that the prompt & model should be changed as needed. For our project, we used the gpt-4o-mini model. It is also possible that the API endpoint is changed in the future, but as of the time of this project, we used the chat completions endpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary files & inputs\n",
    "\n",
    "- A single jsonl file containing all data to be processed.\n",
    "- An output directory for popular jobs (manually reviewed data)\n",
    "- An output directory for unpopular jobs (formatted data to be submitted to OpenAI)\n",
    "- Popular jobs to filter through (change as needed)\n",
    "- A prompt that you would like the AI to read.\n",
    "- Optionally, change chunk size depending on how many divisions you would like data to be (maximum 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Input file paths\n",
    "input_file = \"/Users/will/research/DataRetrieval/filtered_data/combined.jsonl\"\n",
    "\n",
    "# Directories for output\n",
    "popular_dir = \"/Users/will/research/DataRetrieval/splitData/popular\"\n",
    "unpopular_dir = \"/Users/will/research/DataRetrieval/splitData/unpopular\"\n",
    "\n",
    "# Popular titles and their seniority levels\n",
    "popular_titles = {\n",
    "    \"personal banker\": \"Entry\",\n",
    "    \"vice president\": \"Executive\",\n",
    "    \"manager\": \"Mid\",\n",
    "    \"associate\": \"Mid\",\n",
    "    \"teller\": \"Entry\",\n",
    "    \"financial advisor\": \"Mid\",\n",
    "    \"bank teller\": \"Entry\",\n",
    "    \"analyst\": \"Mid\",\n",
    "    \"project manager\": \"Senior\",\n",
    "    \"business analyst\": \"Mid\"\n",
    "}\n",
    "\n",
    "# File limits for unpopular jobs\n",
    "chunk_size = 50000\n",
    "\n",
    "# Initialize counters and buffers\n",
    "unpopular_buffer = []\n",
    "unpopular_file_index = 1\n",
    "\n",
    "# Track occurrences of each employee_id\n",
    "employee_id_counts = {}\n",
    "\n",
    "# System message for preparing unpopular jobs\n",
    "system_message = \"\"\"Your prompt here.\"\"\"\n",
    "\n",
    "def write_unpopular_jobs():\n",
    "    global unpopular_buffer, unpopular_file_index\n",
    "\n",
    "    if not unpopular_buffer:\n",
    "        return\n",
    "\n",
    "    output_file = f\"{unpopular_dir}/unpopular_jobs_{unpopular_file_index}.jsonl\"\n",
    "    print(f\"Writing {len(unpopular_buffer)} unpopular jobs to {output_file}\")\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        for custom_id, record_data in unpopular_buffer:\n",
    "            output_line = {\n",
    "                \"custom_id\": custom_id,\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": \"gpt-4o-mini\",\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": system_message},\n",
    "                        # Write everything except \"employee_id\" in the user content\n",
    "                        {\"role\": \"user\", \"content\": json.dumps(record_data, ensure_ascii=False)}\n",
    "                    ],\n",
    "                    \"max_tokens\": 1000\n",
    "                }\n",
    "            }\n",
    "            f.write(json.dumps(output_line, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    unpopular_file_index += 1\n",
    "    unpopular_buffer = []\n",
    "\n",
    "def get_or_make_employee_id(original_id):\n",
    "    \"\"\"\n",
    "    If this employee_id has been seen before, increment the counter\n",
    "    and produce e.g. 'asdf_2', otherwise store and return as-is.\n",
    "    \"\"\"\n",
    "    if not original_id:\n",
    "        # Fallback if no employee_id is present\n",
    "        return \"no_employee_id\"\n",
    "\n",
    "    if original_id not in employee_id_counts:\n",
    "        employee_id_counts[original_id] = 1\n",
    "        return original_id\n",
    "    else:\n",
    "        employee_id_counts[original_id] += 1\n",
    "        return f\"{original_id}_{employee_id_counts[original_id]}\"\n",
    "\n",
    "# Process the input file\n",
    "try:\n",
    "    with open(input_file, 'r') as infile:\n",
    "        for line_num, line in enumerate(infile, start=1):\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                title_name = data.get(\"title.name\")\n",
    "                if title_name and isinstance(title_name, str):\n",
    "                    title_name = title_name.lower()\n",
    "                else:\n",
    "                    title_name = \"\"\n",
    "\n",
    "                # Extract and prepare custom_id\n",
    "                original_employee_id = data.get(\"employee_id\", \"\")\n",
    "                final_employee_id = get_or_make_employee_id(original_employee_id)\n",
    "                \n",
    "                # Remove employee_id from the content we'll send to \"user\"\n",
    "                # but keep the rest\n",
    "                data_no_id = {k: v for k, v in data.items() if k != \"employee_id\"}\n",
    "\n",
    "                if any(popular_title in title_name for popular_title in popular_titles):\n",
    "                    # Find the matching popular title\n",
    "                    matching_title = next(title for title in popular_titles if title in title_name)\n",
    "                    seniority = popular_titles[matching_title]\n",
    "                    \n",
    "                    output_line = {\n",
    "                        \"custom_id\": final_employee_id,\n",
    "                        \"all results\": [\n",
    "                            {\n",
    "                                \"job_function\": matching_title,\n",
    "                                \"job_function_certainty\": \"manual\",\n",
    "                                \"seniority\": seniority,\n",
    "                                \"seniority_certainty\": \"manual\"\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                    output_file = f\"{popular_dir}/popular_jobs.jsonl\"\n",
    "                    with open(output_file, 'a') as f:\n",
    "                        f.write(json.dumps(output_line) + \"\\n\")\n",
    "                else:\n",
    "                    # Accumulate for unpopular jobs\n",
    "                    # We'll store the custom_id and the data-no-ID together\n",
    "                    unpopular_buffer.append((final_employee_id, data_no_id))\n",
    "\n",
    "                    # Write out if we hit chunk_size\n",
    "                    if len(unpopular_buffer) >= chunk_size:\n",
    "                        write_unpopular_jobs()\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON on line {line_num}: {e}\")\n",
    "\n",
    "    # Write any remaining unpopular jobs\n",
    "    write_unpopular_jobs()\n",
    "\n",
    "    print(\"Processing complete. Files saved to output directories.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Input file not found: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Token Utility\n",
    "\n",
    "One important additional detail to consider is how many tokens your data contains. Everyone using the API has a rate limit, which stops excessive use of the API. While this limit can be increased by additional spending, it is important to consider how many tokens your data takes before submitting.\n",
    "\n",
    "Luckily, OpenAI uses a Python library called \"tiktoken\" in order to tokenize their data, meaning that we can use this same library to get a good estimation of how many tokens our data will take. While we can actually exactly estimate the input tokens, it is impossible to know exactly how many tokens will be in the output. So, I reccomend running small tests and calculating an average token for the response, and basing your output calculations on that. For our data, our respones were always less than 100 tokens per responses, so I used 100 as a worst case per data point. Also, there are some additional \"overhead\" tokens which OpenAI adds to the API calls which we account for (note that these rules only apply to the GPT-4 models, and may likely change in the future, although the overhead is very small). \n",
    "\n",
    "It is reccomended that you be aware of how many tokens your data will use before submitting to OpenAI. Exceeding the daily rate limit will cause the submission, and all subsequent submissions that day, to fail. Also, there is a maximum of 50000 lines in the jsonl input file and a maximum of 200MB of data per file as of the time of writing. Ensure that your rate limit (visible in your OpenAI dashboard) is greater than the sum of the data that you will submit, and that no individual file exceeds 200MB in size.\n",
    "\n",
    "### Input\n",
    "\n",
    "- Provide a directory containing all files you wish to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tiktoken\n",
    "\n",
    "def count_tokens_gpt4(messages):\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    tokens_per_message = 3\n",
    "    tokens_per_name = 1\n",
    "    \n",
    "    total_tokens = 0\n",
    "    \n",
    "    for message in messages:\n",
    "        # Add overhead tokens per message\n",
    "        total_tokens += tokens_per_message\n",
    "        \n",
    "        # Add tokens for each key-value pair\n",
    "        for key, value in message.items():\n",
    "            if key == \"name\":\n",
    "                total_tokens += tokens_per_name\n",
    "            # Ensure we're encoding a string\n",
    "            if isinstance(value, str):\n",
    "                total_tokens += len(encoding.encode(value))\n",
    "            else:\n",
    "                total_tokens += len(encoding.encode(str(value)))\n",
    "                \n",
    "    # Add 3 tokens for the assistant's start-of-reply overhead\n",
    "    total_tokens += 3\n",
    "    \n",
    "    return total_tokens\n",
    "\n",
    "def process_directory(directory_path):\n",
    "    # We assume the worst-case completion might use 100 tokens\n",
    "    max_response_tokens = 100\n",
    "\n",
    "    grand_total_prompt = 0\n",
    "    grand_total_estimated = 0\n",
    "    request_count = 0\n",
    "\n",
    "    # Iterate through all .jsonl files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".jsonl\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            print(f\"Processing file: {filename}\")\n",
    "\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "\n",
    "                    data = json.loads(line)\n",
    "\n",
    "                    # Extract the messages array\n",
    "                    body = data.get(\"body\", {})\n",
    "                    messages = body.get(\"messages\", [])\n",
    "\n",
    "                    # Count the prompt tokens using GPT-4's known rules\n",
    "                    prompt_tokens = count_tokens_gpt4(messages)\n",
    "\n",
    "                    # Our worst-case total usage = prompt_tokens + 100\n",
    "                    estimated_total_tokens = prompt_tokens + max_response_tokens\n",
    "\n",
    "                    # Print line-by-line info\n",
    "                    request_count += 1\n",
    "                    print(f\"  Request {request_count} prompt tokens: {prompt_tokens}\")\n",
    "                    print(f\"  Request {request_count} estimated total with response: {estimated_total_tokens}\\n\")\n",
    "\n",
    "                    # Accumulate grand totals\n",
    "                    grand_total_prompt += prompt_tokens\n",
    "                    grand_total_estimated += estimated_total_tokens\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Total requests processed: {request_count}\")\n",
    "    print(f\"Grand total prompt tokens (across all requests): {grand_total_prompt}\")\n",
    "    print(f\"Grand total estimated tokens (prompt + 100 each): {grand_total_estimated}\")\n",
    "\n",
    "def main():\n",
    "    directory_path = \"path/to/your/directory\"  # Update this path as needed\n",
    "    process_directory(directory_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
